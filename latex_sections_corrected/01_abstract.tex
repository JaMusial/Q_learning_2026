% ========================================================================
% ABSTRACT (CORRECTED VERSION)
% ========================================================================
% Changes:
% - First person ("we present", "we demonstrate")
% - Uppercase symbols Y(t), U(t), lowercase e(t) for error
% - Removed specific numerical claims (12-22%)
% - Clarified projection enables initialization
% - Removed staged learning references
% - Updated: Error symbol changed from E to e throughout
% ========================================================================

\begin{abstract}
The performance of a significant majority of industrial proportional-integral-derivative (PID) controllers remains suboptimal due to inadequate tuning. Manual retuning requires expert knowledge and experimental data, making it impractical for large-scale industrial facilities operating hundreds of control loops simultaneously. This challenge is further compounded when processes exhibit significant dead time, which degrades closed-loop performance and complicates traditional compensation methods such as Smith Predictor or Internal Model Control, both of which require accurate process models.

In this paper, we present a model-free Q-learning controller with two key extensions that address these industrial challenges. First, a projection function enables bumpless initialization when the desired closed-loop time constant $T_e$ differs from the baseline integral time $T_I$ of the existing PI controller. The projection term compensates for this mismatch through an additional control contribution derived from first-order trajectory requirements. Second, a delayed credit assignment mechanism handles dead time compensation by decoupling the physical plant delay $T_0$ from the controller's compensation strategy $T_{0,\text{controller}}$, implemented through FIFO buffers that defer Q-value updates until action consequences become observable.

The proposed Q2d controller bumplessly replaces existing PI controllers, starting with equivalent initial performance through proper initialization of the Q-matrix as an identity matrix and appropriate selection of controller gain $K_Q = K_P$. The controller then gradually improves online through reinforcement learning without requiring a process model or offline training. We validate the approach on first- and second-order inertia processes with dead times up to 4 seconds. Notably, the method exhibits graceful degradation: even 50\% undercompensation of dead time (setting $T_{0,\text{controller}} = T_0/2$) provides substantially better convergence than no compensation, offering robustness to uncertain dead time estimates common in industrial practice.

\end{abstract}

\begin{IEEEkeywords}
Q-learning, dead time compensation, projection function, industrial control, reinforcement learning, bumpless switching, model-free control, delayed credit assignment
\end{IEEEkeywords}
